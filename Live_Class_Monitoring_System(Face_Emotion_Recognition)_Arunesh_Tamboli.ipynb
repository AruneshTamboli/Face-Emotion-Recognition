{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Live Class Monitoring System(Face Emotion Recognition) Arunesh Tamboli.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AruneshTamboli/Face-Emotion-Recognition/blob/main/Live_Class_Monitoring_System(Face_Emotion_Recognition)_Arunesh_Tamboli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQiLYtTOGivr"
      },
      "source": [
        "# **Project Name** \n",
        "\n",
        " **Live Class Monitoring System(Face Emotion Recognition)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOT8t-KwdKtH"
      },
      "source": [
        "# **Group Members**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmpdko7fdPvT"
      },
      "source": [
        "Rohit bhadauriya\n",
        "\n",
        "Arunesh Tamboli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6INDTocGqhl"
      },
      "source": [
        "# **Project Indroduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W26lL4fAGrwI"
      },
      "source": [
        "The Indian education landscape has been undergoing rapid changes for the past 10 years owing to\n",
        "the advancement of web-based learning services, specifically, eLearning platforms.\n",
        "\n",
        "Global E-learning is estimated to witness an 8X over the next 5 years to reach USD 2B in 2021. \n",
        "India\n",
        "is expected to grow with a CAGR of 44% crossing the 10M users mark in 2021. Although the market\n",
        "is growing on a rapid scale, there are major challenges associated with digital learning when\n",
        "compared with brick and mortar classrooms.\n",
        "\n",
        " One of many challenges is how to ensure quality\n",
        "learning for students. Digital platforms might overpower physical classrooms in terms of content\n",
        "quality but when it comes to understanding whether students are able to grasp the content in a live\n",
        "class scenario is yet an open-end challenge.\n",
        "\n",
        "In a physical classroom during a lecturing teacher can see the faces and assess the emotion of the\n",
        "class and tune their lecture accordingly, whether he is going fast or slow. He can identify students who\n",
        "need special attention. \n",
        "\n",
        "Digital classrooms are conducted via video telephony software program (exZoom) where it’s not possible for medium scale class (25-50) to see all students and access the\n",
        "mood. Because of this drawback, students are not focusing on content due to lack of surveillance.\n",
        "\n",
        "While digital platforms have limitations in terms of physical surveillance but it comes with the power of\n",
        "data and machines which can work for you. It provides data in the form of video, audio, and texts\n",
        "which can be analysed using deep learning algorithms.\n",
        "\n",
        " Deep learning backed system not only solves\n",
        "the surveillance issue, but it also removes the human bias from the system, and all information is no\n",
        "longer in the teacher’s brain rather translated in numbers that can be analysed and tracked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bIjM9UWGysg"
      },
      "source": [
        "# **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9oIWThTG6Em"
      },
      "source": [
        "We will solve the above-mentioned challenge by applying deep learning algorithms to live video data.\n",
        "\n",
        "The solution to this problem is by recognizing facial emotions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOVA_nyG-CG"
      },
      "source": [
        "# **What is Face Emotion Recognition ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWlRG8DHHCWe"
      },
      "source": [
        "This is a few shot learning live face emotion detection system. The model should be able to real-time\n",
        "identify the emotions of students in a live class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41dEFZ68HHCL"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgNFtEik5DID",
        "outputId": "89060849-51e0-4ee7-e309-e31412067418"
      },
      "source": [
        "pip install opencv_python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv_python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv_python) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5myHQyVHOLj"
      },
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "### CNN models ###\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\n",
        "from keras.utils import np_utils\n",
        "from keras.regularizers import l2\n",
        "# from keras.optimizers import SGD, RMSprop\n",
        "# from keras.utils import to_categorical\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from keras import models\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.layers import Input, GlobalAveragePooling2D,concatenate\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1psAgWo7HXJN"
      },
      "source": [
        "import sys, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "# from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTIYTRrzIR-3"
      },
      "source": [
        "# **Inspect the Data in First Look**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwju72i-DyIw",
        "outputId": "4047bc41-b8c1-46dd-9c02-c4804d127af1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A65VmNsuIcu2"
      },
      "source": [
        "# Image Reading\n",
        "img_array=cv2.imread(\"/content/drive/MyDrive/Almabetter/Capstone Projects/Deep Learning 29-10-21/fer2013.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "Y8qv1E0yIx6p",
        "outputId": "f4537e7d-15e6-48c8-e021-8f21640d1072"
      },
      "source": [
        "img_array.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ecaaff735b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8y0aqNPIyiQ"
      },
      "source": [
        "plt.imshow(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oWnUXbUPyrt"
      },
      "source": [
        "plt.imshow(cv2.cvtColor(img_array,cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLfxHgZEEr2a"
      },
      "source": [
        "# **1: Directly Using DeepFace**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0grxw0XE0RN"
      },
      "source": [
        "# Upload A file from local Drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVeNXun-E6L7"
      },
      "source": [
        "# Image Show\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "img_array=cv2.imread(\"/content/drive/MyDrive/5-capston project deep learning face emotion detection/Andrew-Ng-anger.jpg\")\n",
        "plt.imshow(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYeGx2aCkanS"
      },
      "source": [
        "pip install deepface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFnSyjMfkOFK"
      },
      "source": [
        "from deepface import DeepFace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRltaxFwQ7X6"
      },
      "source": [
        "predictions=DeepFace.analyze(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyzjWDyUG1Tg"
      },
      "source": [
        "# Analyze Image using DeepFace\n",
        "from deepface import DeepFace\n",
        "obj = DeepFace.analyze(img_path = \"/content/drive/MyDrive/5-capston project deep learning face emotion detection/Andrew-Ng-anger.jpg\", actions = ['age', 'gender', 'race', 'emotion'])\n",
        "print(obj[\"age\"],\" years old \",obj[\"dominant_race\"],\" \",obj[\"dominant_emotion\"],\" \", obj[\"gender\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL-ixNXyHCto"
      },
      "source": [
        "great expression and age limit detect perfectly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEqrye8JHJSq"
      },
      "source": [
        "2 : Transfer Learning Using CPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhS8ibBHWWal"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/5-capston project deep learning face emotion detection/fer2013.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgmNCQ-RWfE7"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoybDQ1kWjqN"
      },
      "source": [
        "## shape of the dataset\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLyrt_NAcYSb"
      },
      "source": [
        "df.emotion.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO7ZDDgGcat7"
      },
      "source": [
        "emotion_label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wHUm3OfXZLh"
      },
      "source": [
        "## checking for null values\n",
        "\n",
        "df.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wTTb5xhXqA7"
      },
      "source": [
        "So,in the dataset no missing values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1tsYGAAXjjZ"
      },
      "source": [
        "df['emotion'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J92kyqracxtb"
      },
      "source": [
        "sns.countplot(df.emotion)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHynzOVCdqjN"
      },
      "source": [
        "math.sqrt(len(df.pixels[0].split(' ')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EopzIJWnbsEG"
      },
      "source": [
        "## **Path of input Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-EOxa9JX6Vr"
      },
      "source": [
        "train_data_dir = '../input/emotion-detection-fer/train'\n",
        "validation_data_dir = '../input/emotion-detection-fer/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apOh-Szgba_z"
      },
      "source": [
        "# **Displaying Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AowjuLK4bSk2"
      },
      "source": [
        "# size of the image: 48*48 pixels\n",
        "\n",
        "fig = pyplot.figure(1, (14, 14))\n",
        "\n",
        "k = 0\n",
        "for label in sorted(df.emotion.unique()):\n",
        "    for j in range(7):\n",
        "        px = df[df.emotion==label].pixels.iloc[k]\n",
        "        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')\n",
        "\n",
        "        k += 1\n",
        "        ax = pyplot.subplot(7, 7, k)\n",
        "        ax.imshow(px, cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_title(emotion_label_to_text[label])\n",
        "        pyplot.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7VWRTVfeZrd"
      },
      "source": [
        "INTERESTED_LABELS = [3, 4, 6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1JMF7U2elny"
      },
      "source": [
        "df = df[df.emotion.isin(INTERESTED_LABELS)]\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsvWPGaof7wn"
      },
      "source": [
        "Now prepare the data compatible for neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzP5mrM1fn4G"
      },
      "source": [
        "img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\n",
        "img_array = np.stack(img_array, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xhy4m6SgI7O"
      },
      "source": [
        "img_array.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptsTg_G6gjQ2"
      },
      "source": [
        "le = LabelEncoder()\n",
        "img_labels = le.fit_transform(df.emotion)\n",
        "img_labels = np_utils.to_categorical(img_labels)\n",
        "img_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZXhyigZgqS7"
      },
      "source": [
        "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(le_name_mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-7tdFT2hCap"
      },
      "source": [
        "Splitting the data into training and validation set.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47UA2GfWg6dV"
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(img_array, img_labels,\n",
        "                                                    shuffle=True, stratify=img_labels,\n",
        "                                                    test_size=0.1, random_state=42)\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI5AcJYehLKM"
      },
      "source": [
        "del df\n",
        "del img_array\n",
        "del img_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1oNrsklhX81"
      },
      "source": [
        "img_width = X_train.shape[1]\n",
        "img_height = X_train.shape[2]\n",
        "img_depth = X_train.shape[3]\n",
        "num_classes = y_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDBK8edMht3s"
      },
      "source": [
        "Normalization is important for neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AedX091thnK4"
      },
      "source": [
        "# Normalizing results, as neural networks are very sensitive to unnormalized data.\n",
        "X_train = X_train / 255.\n",
        "X_valid = X_valid / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-FCW2HDiV6W"
      },
      "source": [
        "def build_net(optim):\n",
        "    \"\"\"\n",
        "    This is a Deep Convolutional Neural Network (DCNN). For generalization purpose I used dropouts in regular intervals.\n",
        "    I used `ELU` as the activation because it avoids dying relu problem but also performed well as compared to LeakyRelu\n",
        "    atleast in this case. `he_normal` kernel initializer is used as it suits ELU. BatchNormalization is also used for better\n",
        "    results.\n",
        "    \"\"\"\n",
        "    net = Sequential(name='DCNN')\n",
        "\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(5,5),\n",
        "            input_shape=(img_width, img_height, img_depth),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_1'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_1'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(5,5),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_2'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_2'))\n",
        "    \n",
        "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n",
        "    net.add(Dropout(0.4, name='dropout_1'))\n",
        "\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3,3),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_3'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_3'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3,3),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_4'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_4'))\n",
        "    \n",
        "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n",
        "    net.add(Dropout(0.4, name='dropout_2'))\n",
        "\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3,3),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_5'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_5'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3,3),\n",
        "            activation='elu',\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='conv2d_6'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_6'))\n",
        "    \n",
        "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n",
        "    net.add(Dropout(0.5, name='dropout_3'))\n",
        "\n",
        "    net.add(Flatten(name='flatten'))\n",
        "        \n",
        "    net.add(\n",
        "        Dense(\n",
        "            128,\n",
        "            activation='elu',\n",
        "            kernel_initializer='he_normal',\n",
        "            name='dense_1'\n",
        "        )\n",
        "    )\n",
        "    net.add(BatchNormalization(name='batchnorm_7'))\n",
        "    \n",
        "    net.add(Dropout(0.6, name='dropout_4'))\n",
        "    \n",
        "    net.add(\n",
        "        Dense(\n",
        "            num_classes,\n",
        "            activation='softmax',\n",
        "            name='out_layer'\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    net.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=optim,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    net.summary()\n",
        "    \n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8YymGL1ikKq"
      },
      "source": [
        "\"\"\"\n",
        "I used two callbacks one is 'early stopping' for avoiding overfitting training data\n",
        "and other `ReduceLROnPlateau` for learning rate.\n",
        "\"\"\"\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0.00005,\n",
        "    patience=11,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=7,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    early_stopping,\n",
        "    lr_scheduler,\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKFt8SRJzjsI"
      },
      "source": [
        "# As the data in hand is less as compared to the task so ImageDataGenerator is good to go.\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "train_datagen.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWZF7rYkA0A2"
      },
      "source": [
        "batch_size = 32 #batch size of 32 performs the best.\n",
        "epochs = 25  #100\n",
        "optims = [\n",
        "    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n",
        "    optimizers.Adam(0.001),\n",
        "]\n",
        "\n",
        "# I tried both `Nadam` and `Adam`, the difference in results is not different but I finally went with Nadam as it is more popular.\n",
        "model = build_net(optims[1]) \n",
        "history = model.fit_generator(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    steps_per_epoch=len(X_train) / batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    use_multiprocessing=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-sbZf5PqHUx"
      },
      "source": [
        "model_yaml = model.to_json()\n",
        "with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "     yaml_file.write(model_yaml)\n",
        "    \n",
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEBfU_shqeme"
      },
      "source": [
        "sns.set()\n",
        "fig = pyplot.figure(0, (12, 4))\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 1)\n",
        "sns.lineplot(history.epoch, history.history['accuracy'], label='train')\n",
        "sns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 2)\n",
        "sns.lineplot(history.epoch, history.history['loss'], label='train')\n",
        "sns.lineplot(history.epoch, history.history['val_loss'], label='valid')\n",
        "pyplot.title('Loss')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "pyplot.savefig('epoch_history_dcnn.png')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4KlMC8bq2XA"
      },
      "source": [
        "The epochs history shows that accuracy gradually increases and achieved +83% accuracy on both training and validation set, but at the end the model starts overfitting training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3-Nk-2qrNkw"
      },
      "source": [
        "df_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})\n",
        "df_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})\n",
        "\n",
        "fig = pyplot.figure(0, (14, 4))\n",
        "ax = pyplot.subplot(1, 2, 1)\n",
        "sns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 2)\n",
        "sns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\n",
        "pyplot.title('Loss')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "pyplot.savefig('performance_dist.png')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhd7SHT_unW9"
      },
      "source": [
        "!pip install scikit-plot\n",
        "from scikitplot.estimators import plot_feature_importances\n",
        "from scikitplot.metrics import plot_confusion_matrix, plot_roc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV9ODRqaurS7"
      },
      "source": [
        "because next cell showing an error for scikit plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfUAoJM_rOZt"
      },
      "source": [
        "# from scikitplot.estimators import plot_feature_importances\n",
        "# from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
        "# yhat_valid = model.predict(X_valid)\n",
        "# yhat_valid = np.round(yhat_valid).astype(int)\n",
        "# # yhat_valid = model.predict_classes(X_valid)\n",
        "# scikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid, figsize=(7,7))\n",
        "# pyplot.savefig(\"confusion_matrix_dcnn.png\")\n",
        "\n",
        "# print(f'total wrong validation predictions: {np.sum(np.argmax(y_valid, axis=1) != yhat_valid)}\\n\\n')\n",
        "# print(classification_report(np.argmax(y_valid, axis=1), yhat_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku6gpHjrtO47"
      },
      "source": [
        "The confusion matrix clearly shows that our model is doing good job on the class `happy` but it's performance is low on other two classes. One of the reason for this could be the fact that these two classes have less data. But when I looked at the images I found some images from these two classes are even hard for a human to tell whether the person is sad or neutral. Facial expression depends on individual as well. Some person's neutral face looks like sad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xufqn3fArVd-"
      },
      "source": [
        "mapper = {\n",
        "    0: \"happy\",\n",
        "    1: \"sad\",\n",
        "    2: \"neutral\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3YjKHRFtUmd"
      },
      "source": [
        "np.random.seed(2)\n",
        "random_sad_imgs = np.random.choice(np.where(y_valid[:, 1]==1)[0], size=9)\n",
        "random_neutral_imgs = np.random.choice(np.where(y_valid[:, 2]==1)[0], size=9)\n",
        "\n",
        "fig = pyplot.figure(1, (18, 4))\n",
        "\n",
        "for i, (sadidx, neuidx) in enumerate(zip(random_sad_imgs, random_neutral_imgs)):\n",
        "        ax = pyplot.subplot(2, 9, i+1)\n",
        "        sample_img = X_valid[sadidx,:,:,0]\n",
        "        ax.imshow(sample_img, cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        y_predict = np.argmax(model.predict(sample_img.reshape(1,48,48,1)), axis=-1)\n",
        "        ax.set_title(f\"true:sad, pred:{mapper[y_predict[0]]}\")\n",
        "        \n",
        "        ax = pyplot.subplot(2, 9, i+10)\n",
        "        sample_img = X_valid[neuidx,:,:,0]\n",
        "        ax.imshow(sample_img, cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        y_ct = np.argmax(model.predict(sample_img.reshape(1,48,48,1)), axis=-1)\n",
        "        ax.set_title(f\"t:neut, p:{mapper[y_ct[0]]}\")\n",
        "\n",
        "        pyplot.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIWB47pafEvG"
      },
      "source": [
        "# **Real Time Video Face Detection Using Local Webcam**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCSyr2Jve_gS"
      },
      "source": [
        "## **Detect Single Face**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7O7krmiv5X"
      },
      "source": [
        "import cv2\n",
        "#from deepface import DeepFace\n",
        "\n",
        "\n",
        "new_model=tf.keras.models.load_model('model.h5')\n",
        "path= \"/content/drive/MyDrive/5-capston project deep learning face emotion detection/haarcascade_frontalface_default.xml\"\n",
        "font_scale=1.5\n",
        "font=cv2.FONT_HERSHEY_PLAIN\n",
        "\n",
        "#set the rectangle background to white\n",
        "rectangle_bgr=(255,255,255)\n",
        "#make a black image\n",
        "img=np.zeros((500,500))\n",
        "#set some text\n",
        "text=\"Some text in a box!\"\n",
        "#get the width and height of the tesxt box\n",
        "(text_width, text_height)=cv2.getTextSize(text, font, fontScale=font_scale, thickness=1)[0]\n",
        "#set the text start position\n",
        "text_offset_x=10\n",
        "text_offset_y=img.shape[0]-25\n",
        "#make the coords of the box with a small padding of two pixels\n",
        "box_coords=((text_offset_x,text_offset_y),(text_offset_x+text_width, text_offset_y-text_height-2))\n",
        "cv2.rectangle(img,box_coords[0],box_coords[1],rectangle_bgr, cv2.FILLED)\n",
        "cv2.putText(img,text,(text_offset_x,text_offset_y),font, fontScale=font_scale, color=(0,0,0),thickness=1)\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "img_counter = 0\n",
        "if not cap.isOpened():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    img_counter = 0 \n",
        "if not cap.isOpened():\n",
        "    raise IOError(\"Cannot use webcam\")\n",
        "\n",
        "\n",
        "while True:\n",
        "    ret,frame=cap.read()\n",
        "    #eye_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "    faceCascade=cv2.CascadeClassifier(cv2.data.haarcascades + path)\n",
        "    gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces= faceCascade.detectMultiScale(gray,1.1,4)\n",
        "    for x,y,w,h in faces:\n",
        "        roi_gray = gray[y:y+h,x:x+w]\n",
        "        roi_color=frame[y:y+h, x:x+w]\n",
        "        roi_gray=cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
        "        cv2.rectangle(frame, (x,y),(x+w,y+h+12),(255,0,0),2)\n",
        "        facess=faceCascade.detectMultiScale(roi_gray)\n",
        "        if len(facess)==0:\n",
        "            print(\"Face not detected\")\n",
        "        else:\n",
        "            for(ex,ey,ew,eh) in facess:\n",
        "                face_roi= roi_color[ey:ey+eh,ex:ex+ew] ## Cropping The Face\n",
        "\n",
        "\n",
        "    \n",
        "    final_image=cv2.resize(face_roi,(224,224))\n",
        "    final_image=np.expand_dims(final_image,axis=0)\n",
        "    final_image=final_image/255.0\n",
        "    \n",
        "    font=cv2.FONT_HERSHEY_SIMPLEX\n",
        "    \n",
        "    Predictions=new_model.predict(final_image)\n",
        "    \n",
        "    font_scale=1.5\n",
        "    font=cv2.FONT_HERSHEY_PLAIN\n",
        "    \n",
        "    \n",
        "    if(np.argmax(Predictions)==0):\n",
        "        status=\"Angry\"\n",
        "        x1,y1,w1,h1= 0,0,175,75\n",
        "        #Draw black background rectangle\n",
        "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
        "        #Add Text\n",
        "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),font,0.7,(0,0,255),2)\n",
        "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
        "    \n",
        "    \n",
        "    \n",
        "    elif(np.argmax(Predictions)==1):\n",
        "        status=\"Disgust\"\n",
        "        x1,y1,w1,h1= 0,0,175,75\n",
        "        #Draw black background rectangle\n",
        "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
        "        #Add Text\n",
        "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),font,0.7,(0,0,255),2)\n",
        "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
        "    \n",
        "    elif(np.argmax(Predictions)==2):\n",
        "        status=\"Fear\"\n",
        "        x1,y1,w1,h1= 0,0,175,75\n",
        "        #Draw black background rectangle\n",
        "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
        "        #Add Text\n",
        "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),font,0.7,(0,0,255),2)\n",
        "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
        "    \n",
        "    elif(np.argmax(Predictions)==3):\n",
        "        status=\"Happy\"\n",
        "        x1,y1,w1,h1= 0,0,175,75\n",
        "        #Draw black background rectangle\n",
        "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
        "        #Add Text\n",
        "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),font,0.7,(0,0,255),2)\n",
        "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
        "    \n",
        "    elif(np.argmax(Predictions)==4):\n",
        "        status=\"Sad\"\n",
        "        x1,y1,w1,h1= 0,0,175,75\n",
        "        #Draw black background rectangle\n",
        "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
        "        #Add Text\n",
        "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),font,0.7,(0,0,255),2)\n",
        "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
        "    \n",
        "    elif(np.argmax(Predictions)==5):\n",
        "        status=\"Surprise\"\n",
        "        x1,y1,w1,h1= 0,0,175,75\n",
        "        #Draw black background rectangle\n",
        "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
        "        #Add Text\n",
        "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),font,0.7,(0,0,255),2)\n",
        "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
        "    \n",
        "    \n",
        "    elif(np.argmax(Predictions)==6):\n",
        "        status=\"Neutral\"\n",
        "        x1,y1,w1,h1= 0,0,175,75\n",
        "        #Draw black background rectangle\n",
        "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
        "        #Add Text\n",
        "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),font,0.7,(0,0,255),2)\n",
        "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
        "                    \n",
        "    \n",
        "                    \n",
        "    cv2.imshow('Face Emotion Recognition',frame)\n",
        "    if cv2.waitKey(2) & 0xFF==ord('q'):\n",
        "        break\n",
        "\n",
        "    \n",
        "    \n",
        "            \n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT-FnakVUqWb"
      },
      "source": [
        "## **Detect Multiple Faces**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quQHszF8Fluw"
      },
      "source": [
        "from keras.models import load_model\n",
        "from time import sleep\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing import image\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') # Face Detection\n",
        "classifier =load_model('model.h5')  #Load model\n",
        "\n",
        "emotion_labels = ['Angry','Disgust','Fear','Happy','Neutral', 'Sad', 'Surprise']  # Emotion that will be predicted\n",
        "\n",
        "cap = cv2.VideoCapture(0)  ## Opening webcam\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    _, frame = cap.read()\n",
        "    labels = []\n",
        "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "    # plt.imshow(cv2.cvtColor(img_array,cv2.COLOR_BGR2RGB))\n",
        "    faces = face_classifier.detectMultiScale(gray)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,255),2)\n",
        "        roi_gray = gray[y:y+h,x:x+w]\n",
        "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)  ##Face Cropping for prediction\n",
        "\n",
        "\n",
        "\n",
        "        if np.sum([roi_gray])!=0:\n",
        "            roi = roi_gray.astype('float')/255.0\n",
        "            roi = img_to_array(roi)\n",
        "            roi = np.expand_dims(roi,axis=0) ## reshaping the cropped face image for prediction\n",
        "\n",
        "            prediction = classifier.predict(roi)[0]   #Prediction\n",
        "            label=emotion_labels[prediction.argmax()]\n",
        "            label_position = (x,y)\n",
        "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)   # Text Adding\n",
        "        else:\n",
        "            cv2.putText(frame,'No Faces',(30,80),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
        "    cv2.imshow('Emotion Detector',frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6Ra3jf4WT5h"
      },
      "source": [
        "# **Real Time Video Detection Using Colab Webcam**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wRzTE4m0vpN"
      },
      "source": [
        "# Save best Model\n",
        "model.save('final_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20sGVqGx0w_f"
      },
      "source": [
        "# Saving model in json format along with weights\n",
        "fer_json = model.to_json()  \n",
        "with open(\"fer.json\", \"w\") as json_file:  \n",
        "    json_file.write(fer_json)  \n",
        "model.save_weights(\"fer.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49z6fH_qU7y8"
      },
      "source": [
        "# import dependencies\n",
        "\n",
        "from IPython.display import display, Javascript, Image,clear_output\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMXEnzyuWZQO"
      },
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4U3mSHWX7ar"
      },
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsvPLXHzYFMe"
      },
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFn5vwbjYy1u"
      },
      "source": [
        "def face_detect():\n",
        "  import cv2\n",
        "  from PIL import Image\n",
        "  import numpy as np\n",
        "  import os\n",
        "  from keras.models import load_model\n",
        "  from time import sleep\n",
        "  from keras.preprocessing.image import img_to_array\n",
        "  from keras.preprocessing import image\n",
        "# start streaming video from webcam\n",
        "  video_stream()\n",
        "# label for video\n",
        "  label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "  bbox = ''\n",
        "  count = 0\n",
        "  face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + path) # Face Detection\n",
        "  classifier =load_model('model.h5')  #Load model\n",
        "  emotion_labels = ['Angry','Disgust','Fear','Happy','Neutral', 'Sad', 'Surprise'] \n",
        "\n",
        "  while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "      break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # get face region coordinates\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # get face bounding box for overlay\n",
        "    for (x,y,w,h) in faces:\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "      roi_gray = gray[y:y+h,x:x+w]\n",
        "      roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)  ##Face Cropping for prediction\n",
        "      if np.sum([roi_gray])!=0:\n",
        "        roi = roi_gray.astype('float')/255.0\n",
        "        roi = img_to_array(roi)\n",
        "        roi = np.expand_dims(roi,axis=0) ## reshaping the cropped face image for prediction\n",
        "        prediction = classifier.predict(roi)[0]   #Prediction\n",
        "        label=emotion_labels[prediction.argmax()]\n",
        "        label_position = (x,y)\n",
        "        cv2.putText(bbox_array,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)   # Text Adding\n",
        "      else:\n",
        "        cv2.putText(bbox_array,'No Faces',(30,80),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-e2eM23ZUhD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}